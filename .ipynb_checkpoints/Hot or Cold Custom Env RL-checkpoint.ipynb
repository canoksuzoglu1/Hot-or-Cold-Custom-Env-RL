{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1afcf49",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced6ce4",
   "metadata": {},
   "source": [
    "## 2. Building an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hot_or_Cold_Env(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: left, right, forward, backward, and diagonal directions\n",
    "        self.action_space = Discrete(8)  # 4 basic directions and 4 diagonal ones\n",
    "        \n",
    "        # Observation space: 100x100 grid\n",
    "        self.observation_space = Box(low=0, high=99, shape=(2,), dtype=np.int32)\n",
    "       \n",
    "        # Randomly initialize the agent's starting position within the 100x100 grid\n",
    "        self.state = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])\n",
    "        \n",
    "        # Set the target position randomly\n",
    "        self.target = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])\n",
    "        \n",
    "        # Ensure the target is not in the same position as the agent\n",
    "        while np.array_equal(self.state, self.target):\n",
    "            self.target = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])\n",
    "\n",
    "        # Set the maximum time period (in steps)\n",
    "        self.period_duration = 100\n",
    "        \n",
    "        # Initialize step count\n",
    "        self.steps_taken = 0\n",
    "        \n",
    "        # Initialize previous distance to an infinite value\n",
    "        self.previous_distance_to_target = np.inf\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply the action based on the index\n",
    "        if action == 0:  # move down\n",
    "            self.state[1] = max(0, self.state[1] - 1)  # decrement y-coordinate\n",
    "        elif action == 1:  # move right\n",
    "            self.state[0] = min(99, self.state[0] + 1)  # increment x-coordinate\n",
    "        elif action == 2:  # move up\n",
    "            self.state[1] = min(99, self.state[1] + 1)  # increment y-coordinate\n",
    "        elif action == 3:  # move left\n",
    "            self.state[0] = max(0, self.state[0] - 1)  # decrement x-coordinate\n",
    "        elif action == 4:  # move down-right (diagonal)\n",
    "            self.state[0] = min(99, self.state[0] + 1)\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 5:  # move up-right (diagonal)\n",
    "            self.state[0] = min(99, self.state[0] + 1)\n",
    "            self.state[1] = min(99, self.state[1] + 1)\n",
    "        elif action == 6:  # move down-left (diagonal)\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 7:  # move up-left (diagonal)\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "            self.state[1] = min(99, self.state[1] + 1)\n",
    "        \n",
    "        # Reduce the remaining time steps\n",
    "        self.period_duration -= 1 \n",
    "        \n",
    "        # Increment the step count\n",
    "        self.steps_taken += 1\n",
    "        \n",
    "        # Calculate the base reward\n",
    "        reward = -0.1  # Small negative reward for each step taken\n",
    "        \n",
    "        # Calculate the Euclidean distance to the target\n",
    "        distance_to_target = np.linalg.norm(self.state - self.target)\n",
    "        \n",
    "        # Check if the agent has reached the target\n",
    "        if np.array_equal(self.state, self.target):\n",
    "            reward += 100  # Large positive reward for reaching the target\n",
    "            done = True  # End the episode\n",
    "        else:\n",
    "            done = False  # Episode continues\n",
    "            \n",
    "            # Check if the agent is getting closer to the target\n",
    "            if self.steps_taken > 1:  # Skip this check for the first step\n",
    "                if distance_to_target < self.previous_distance_to_target:\n",
    "                    reward += 1  # Positive reward for getting closer\n",
    "                else:\n",
    "                    reward -= 0.5  # Penalty for getting further away\n",
    "            \n",
    "        # Update the previous distance to the target\n",
    "        self.previous_distance_to_target = distance_to_target  \n",
    "        \n",
    "        # Check if the episode has timed out\n",
    "        if self.period_duration <= 0:\n",
    "            done = True\n",
    "        \n",
    "        # Set an empty info dictionary (can be used for debugging)\n",
    "        info = {}\n",
    "        \n",
    "        # Return the new state, reward, done flag, and info\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agent's position randomly within the 100x100 grid\n",
    "        self.state = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])\n",
    "        \n",
    "        # Randomly set a new target position within the grid\n",
    "        self.target = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])  \n",
    "        \n",
    "        # Ensure the target is not in the same position as the agent\n",
    "        while np.array_equal(self.state, self.target):\n",
    "            self.target = np.array([np.random.randint(0, 100), np.random.randint(0, 100)])\n",
    "\n",
    "        # Reset the period duration to 100 steps\n",
    "        self.period_duration = 100  \n",
    "        \n",
    "        # Reset the step count\n",
    "        self.steps_taken = 0  \n",
    "        \n",
    "        # Reset the previous distance to an infinite value\n",
    "        self.previous_distance_to_target = np.inf  \n",
    "        \n",
    "        # Return the initial state\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f1112",
   "metadata": {},
   "source": [
    "## 3.Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f37fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Hot_or_Cold_Env class definition should be placed here.\n",
    "\n",
    "# Test function for the Hot or Cold environment\n",
    "def test_hot_or_cold_env():\n",
    "    env = Hot_or_Cold_Env()  # Create an instance of the environment\n",
    "    state = env.reset()  # Reset the environment to get the initial state\n",
    "    \n",
    "    print(\"Initial State:\", state)\n",
    "    print(\"Target:\", env.target)\n",
    "\n",
    "    total_reward = 0  # Variable to track the total reward\n",
    "    for step in range(10):  # Perform a maximum of 10 steps\n",
    "        action = np.random.choice(4)  # Randomly select an action\n",
    "        next_state, reward, done, info = env.step(action)  # Take the action and get the next state and reward\n",
    "\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Print the current state and reward information\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Action: {action}\")\n",
    "        print(f\"  New State: {next_state}\")\n",
    "        print(f\"  Reward: {reward}\")\n",
    "        print(f\"  Total Reward: {total_reward}\")\n",
    "        print(f\"  Distance to Target: {np.linalg.norm(next_state - env.target)}\")\n",
    "        \n",
    "        if done:  # Check if the episode is done\n",
    "            print(\"Agent reached the target or time has run out.\")\n",
    "            break  # Exit the loop if done\n",
    "\n",
    "# Run the test\n",
    "test_hot_or_cold_env()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
