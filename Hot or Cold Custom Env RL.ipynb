{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcf481d",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ca0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602adfb0",
   "metadata": {},
   "source": [
    "## 2. Building an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a8a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hot_or_Cold_Env(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: left, right, forward, backward\n",
    "        self.action_space = Discrete(8)  # 4 temel ve 4 çapraz yön\n",
    "        \n",
    "        # Observation space: 50x50 grid\n",
    "        self.observation_space = Box(low=0, high=49, shape=(2,), dtype=np.int32)\n",
    "       \n",
    "        # Randomly initialize the agent's starting position within the 50x50 grid\n",
    "        self.state = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])\n",
    "        \n",
    "        # Set the target position randomly\n",
    "        self.target = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])\n",
    "        \n",
    "        # Ensure the target is not in the same position as the agent\n",
    "        while np.array_equal(self.state, self.target):\n",
    "            self.target = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])\n",
    "\n",
    "        # Set the period duration (in seconds)\n",
    "        self.period_duration = 100\n",
    "        \n",
    "        # Initialize step count\n",
    "        self.steps_taken = 0\n",
    "        \n",
    "        # Initialize previous distance\n",
    "        self.previous_distance_to_target = np.inf\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        if action == 0:  # backward\n",
    "            self.state[1] = max(0, self.state[1] - 1)  # y ekseninde -1\n",
    "        elif action == 1:  # right\n",
    "            self.state[0] = min(49, self.state[0] + 1)  # x ekseninde +1\n",
    "        elif action == 2:  # forward\n",
    "            self.state[1] = min(49, self.state[1] + 1)  # y ekseninde +1\n",
    "        elif action == 3:  # left\n",
    "            self.state[0] = max(0, self.state[0] - 1)  # x ekseninde -1\n",
    "        elif action == 4:  # right-forward (diagonal)\n",
    "            self.state[0] = min(49, self.state[0] + 1)\n",
    "            self.state[1] = min(49, self.state[1] + 1)\n",
    "        elif action == 5:  # right-backward (diagonal)\n",
    "            self.state[0] = min(49, self.state[0] + 1)\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        elif action == 6:  # left-forward (diagonal)\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "            self.state[1] = min(49, self.state[1] + 1)\n",
    "        elif action == 7:  # left-backward (diagonal)\n",
    "            self.state[0] = max(0, self.state[0] - 1)\n",
    "            self.state[1] = max(0, self.state[1] - 1)\n",
    "        \n",
    "        # Reduce game length by 1 second\n",
    "        self.period_duration -= 1 \n",
    "        \n",
    "        # Increment step count\n",
    "        self.steps_taken += 1\n",
    "        \n",
    "        # Calculate the reward\n",
    "        reward = -0.1  # Small negative reward for each step taken\n",
    "        \n",
    "        # Calculate distance to the target\n",
    "        distance_to_target = np.linalg.norm(self.state - self.target)\n",
    "        \n",
    "        # Check if the agent reached the target\n",
    "        if np.array_equal(self.state, self.target):\n",
    "            reward += 100  # Large positive reward for reaching the target\n",
    "            done = True  # End the episode\n",
    "        else:\n",
    "            done = False  # Not done yet\n",
    "            \n",
    "            # Update previous distance\n",
    "            if self.steps_taken > 1:  # Skip this check for the first step\n",
    "                if distance_to_target < self.previous_distance_to_target:\n",
    "                    reward += 1  # Positive reward for getting closer\n",
    "                else:\n",
    "                    reward -= 0.5  # Negative reward for getting further away\n",
    "            \n",
    "        # Update previous distance\n",
    "        self.previous_distance_to_target = distance_to_target  \n",
    "        \n",
    "        # Check if the game is done\n",
    "        if self.period_duration <= 0:\n",
    "            done = True\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        self.state = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])\n",
    "        self.target = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])  # New target\n",
    "        \n",
    "        # Ensure the target is not in the same position as the agent\n",
    "        while np.array_equal(self.state, self.target):\n",
    "            self.target = np.array([np.random.randint(0, 50), np.random.randint(0, 50)])\n",
    "\n",
    "        self.period_duration = 100  # Reset period duration\n",
    "        self.steps_taken = 0  # Reset step count\n",
    "        self.previous_distance_to_target = np.inf  # Initialize previous distance\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ee8fc",
   "metadata": {},
   "source": [
    "## 3.Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f37fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [22 34]\n",
      "Target: [29 22]\n",
      "Step 1:\n",
      "  Action: 0\n",
      "  New State: [22 33]\n",
      "  Reward: -0.1\n",
      "  Total Reward: -0.1\n",
      "  Distance to Target: 13.038404810405298\n",
      "Step 2:\n",
      "  Action: 1\n",
      "  New State: [23 33]\n",
      "  Reward: 0.9\n",
      "  Total Reward: 0.8\n",
      "  Distance to Target: 12.529964086141668\n",
      "Step 3:\n",
      "  Action: 0\n",
      "  New State: [23 32]\n",
      "  Reward: 0.9\n",
      "  Total Reward: 1.7000000000000002\n",
      "  Distance to Target: 11.661903789690601\n",
      "Step 4:\n",
      "  Action: 3\n",
      "  New State: [22 32]\n",
      "  Reward: -0.6\n",
      "  Total Reward: 1.1\n",
      "  Distance to Target: 12.206555615733702\n",
      "Step 5:\n",
      "  Action: 1\n",
      "  New State: [23 32]\n",
      "  Reward: 0.9\n",
      "  Total Reward: 2.0\n",
      "  Distance to Target: 11.661903789690601\n",
      "Step 6:\n",
      "  Action: 1\n",
      "  New State: [24 32]\n",
      "  Reward: 0.9\n",
      "  Total Reward: 2.9\n",
      "  Distance to Target: 11.180339887498949\n",
      "Step 7:\n",
      "  Action: 2\n",
      "  New State: [24 33]\n",
      "  Reward: -0.6\n",
      "  Total Reward: 2.3\n",
      "  Distance to Target: 12.083045973594572\n",
      "Step 8:\n",
      "  Action: 3\n",
      "  New State: [23 33]\n",
      "  Reward: -0.6\n",
      "  Total Reward: 1.6999999999999997\n",
      "  Distance to Target: 12.529964086141668\n",
      "Step 9:\n",
      "  Action: 0\n",
      "  New State: [23 32]\n",
      "  Reward: 0.9\n",
      "  Total Reward: 2.5999999999999996\n",
      "  Distance to Target: 11.661903789690601\n",
      "Step 10:\n",
      "  Action: 2\n",
      "  New State: [23 33]\n",
      "  Reward: -0.6\n",
      "  Total Reward: 1.9999999999999996\n",
      "  Distance to Target: 12.529964086141668\n"
     ]
    }
   ],
   "source": [
    "# The Hot_or_Cold_Env class definition should be placed here.\n",
    "\n",
    "# Test function for the Hot or Cold environment\n",
    "def test_hot_or_cold_env():\n",
    "    env = Hot_or_Cold_Env()  # Create an instance of the environment\n",
    "    state = env.reset()  # Reset the environment to get the initial state\n",
    "    \n",
    "    print(\"Initial State:\", state)\n",
    "    print(\"Target:\", env.target)\n",
    "\n",
    "    total_reward = 0  # Variable to track the total reward\n",
    "    for step in range(10):  # Perform a maximum of 10 steps\n",
    "        action = np.random.choice(4)  # Randomly select an action\n",
    "        next_state, reward, done, info = env.step(action)  # Take the action and get the next state and reward\n",
    "\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # Print the current state and reward information\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Action: {action}\")\n",
    "        print(f\"  New State: {next_state}\")\n",
    "        print(f\"  Reward: {reward}\")\n",
    "        print(f\"  Total Reward: {total_reward}\")\n",
    "        print(f\"  Distance to Target: {np.linalg.norm(next_state - env.target)}\")\n",
    "        \n",
    "        if done:  # Check if the episode is done\n",
    "            print(\"Agent reached the target or time has run out.\")\n",
    "            break  # Exit the loop if done\n",
    "\n",
    "# Run the test\n",
    "test_hot_or_cold_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f04d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
